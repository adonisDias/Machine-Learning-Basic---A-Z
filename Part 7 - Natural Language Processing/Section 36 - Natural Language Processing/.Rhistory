print("hello world")
setwd("D:/Google Drive/Cursos/Udemy/Machine Learning A-Z/Part 7 - Natural Language Processing/Section 36 - Natural Language Processing")
install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
dataset = read.delim('Restaurant_Reviews.tsv',
quote ='',  # ignoring the quotes
stringsAsFactors = FALSE) # cada review não é uma entidade única, por isso não definimos como factors
# Cleaning the texts
#install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus,
content_transformer(tolower))
View(corpus)
View(corpus)
as.character(corpus[[1]])
corpus = tm_map(corpus,
removeNumbers)
as.character(corpus[[841]])
corpus = tm_map(corpus,
removePunctuation))
corpus = tm_map(corpus,
removePunctuation)
as.character(corpus[[1]])
install.packages('SnowballC')
library(SnowballC) #stopwords
corpus = tm_map(corpus,
removeWords, stopwords())
as.character(corpus[[1]])
corpus = tm_map(corpus,
stemDocument)
as.character(corpus[[1]])
corpus = tm_map(corpus,
stripWhitespace)
as.character(corpus[[841]])
dtm = DocumentTermMatrix(corpus)
View(dtm)
View(dtm)
dtm
dtm = removeSparseTerms(dtm,
0.99)
View(dtm)
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm,
0.999)
View(dtm)
corpus = VCorpus(VectorSource(dataset$Review))
corpus = tm_map(corpus,
content_transformer(tolower))
corpus = tm_map(corpus,
removeNumbers)
corpus = tm_map(corpus,
removePunctuation)
corpus = tm_map(corpus,
removeWords, stopwords())
corpus = tm_map(corpus,
stemDocument)
corpus = tm_map(corpus,
stripWhitespace)
# Creating the bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm,
0.999)
View(dtm)
