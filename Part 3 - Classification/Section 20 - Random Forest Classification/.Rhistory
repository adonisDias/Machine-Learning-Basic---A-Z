print("hello world")
setwd("D:/Google Drive/Cursos/Udemy/Machine Learning A-Z/Part 3 - Classification/Section 18 - Naive Bayes")
library(e1071)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
classifier = naiveBayes(x = training_set[-3],
y = training_Set$Purchased)  # nós precisamos retirar da matrix training_set os valores referentes à varipavel dependente. na definição da classe é indicado que o parâmetro x é "a numeric matrix, or a dataframe of categorical and/or numeric values"
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
classifier = naiveBayes(x = training_set[-3],
y = training_Set$Purchased)  # nós precisamos retirar da matrix training_set os valores referentes à varipavel dependente. na definição da classe é indicado que o parâmetro x é "a numeric matrix, or a dataframe of categorical and/or numeric values"
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
classifier = naiveBayes(x = training_set[-3],
y = training_Set$Purchased)  # nós precisamos retirar da matrix training_set os valores referentes à varipavel dependente. na definição da classe é indicado que o parâmetro x é "a numeric matrix, or a dataframe of categorical and/or numeric values"
classifier = naiveBayes(x = training_set[-3],
y = training_set$Purchased)  # nós precisamos retirar da matrix training_set os valores referentes à varipavel dependente. na definição da classe é indicado que o parâmetro x é "a numeric matrix, or a dataframe of categorical and/or numeric values"
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[, 3], y_pred)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
# Fitting the Naive Bayes Model to the dataset
# install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-3],
y = training_set$Purchased)  # nós precisamos retirar da matrix training_set os valores referentes à varipavel dependente. na definição da classe é indicado que o parâmetro x é "a numeric matrix, or a dataframe of categorical and/or numeric values"
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
y_pred
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Encoding the target feature (categorial variable) as factor, isso pe necessário para resvolver a questão do erro factor(0)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
classifier = naiveBayes(x = training_set[-3],
y = training_set$Purchased)  # nós precisamos retirar da matrix training_set os valores referentes à varipavel dependente. na definição da classe é indicado que o parâmetro x é "a numeric matrix, or a dataframe of categorical and/or numeric values"
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
#antes do tratamento de y_pred, se executarmos ele no console, teremos um resultado de factor(0) ou seja, um vetor de tamanho zero, porque o naive bayse não reconhece a categorical variable as a factor.
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
cm
y_pred
library(cowplot)#library(ElemSatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Naive Bayes (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Naive Bayes (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("D:/Google Drive/Cursos/Udemy/Machine Learning A-Z/Part 3 - Classification/Section 19 - Decision Tree Classification")
setwd("D:/Google Drive/Cursos/Udemy/Machine Learning A-Z/Part 3 - Classification/Section 19 - Decision Tree Classification")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Encoding the target feature (categorial variable) as factor, isso pe necessário para resvolver a questão do erro factor(0)
#dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
y_pred = predict(classifier, newdata = test_set[-3])
y_pred = predict(classifier, newdata = test_set[-3], type = 'class')  # incluído type = 'class' para queu o y_pred não exiba as probabilidades, mas sim as classificações 0 ou 1
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Encoding the target feature (categorial variable) as factor, isso pe necessário para resvolver a questão do erro factor(0)
#dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
# Fitting the Regression Model to the dataset
# install.packages(`rpart`)
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
y_pred = predict(classifier
, newdata = test_set[-3]
, type = 'class')  # incluído type = 'class' para queu o y_pred não exiba as probabilidades, mas sim as classificações 0 ou 1
y_pred = predict(classifier
, newdata = test_set[-3]
, type = 'class')  # incluído type = 'class' para queu o y_pred não exiba as probabilidades, mas sim as classificações 0 ou 1
classifier = rpart(formula = Purchased ~ .,
data = training_set,
method = 'class')
# Predicting the Test set results
y_pred = predict(classifier
, newdata = test_set[-3]
, type = 'class')  # incluído type = 'class' para queu o y_pred não exiba as probabilidades, mas sim as classificações 0 ou 1
y_pred
cm = table(test_set[, 3], y_pred)
cm
library(cowplot)#library(ElemSatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class') # incluído type = 'class' para queu o y_pred não exiba as probabilidades, mas sim as classificações 0 ou 1
plot(set[, 3],
main = 'Decision Tree Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = 'class') # incluído type = 'class' para queu o y_pred não exiba as probabilidades, mas sim as classificações 0 ou 1
plot(set[, 3],
main = 'Decision Tree Classification (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Encoding the target feature (categorial variable) as factor, isso pe necessário para resvolver a questão do erro factor(0)
#dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set,
method = 'class')
y_pred = predict(classifier
, newdata = test_set[-3]
, type = 'class')  # incluído type = 'class' para queu o y_pred não exiba as probabilidades, mas sim as classificações 0 ou 1
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
cm
plot(classifier)
text(classifier)
plot(classifier)
text(classifier)
setwd("D:/Google Drive/Cursos/Udemy/Machine Learning A-Z/Part 3 - Classification/Section 20 - Random Forest Classification")
install.packages('randomForest')
library(randomForest)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Encoding the target feature (categorial variable) as factor, isso pe necessário para resvolver a questão do erro factor(0)
#dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
library("caTools", lib.loc="~/R/win-library/3.5")
View(dataset)
View(dataset)
View(training_set)
classifier = randomForest(x = training_set[-3],
y = training_set$Purchased,
ntree = 10)
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[, 3], y_pred)
cm
cm
cm = table(test_set[, 3], y_pred)
cm
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Encoding the target feature (categorial variable) as factor, isso pe necessário para resvolver a questão do erro factor(0)
#dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
#library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
classifier = randomForest(x = training_set[-3],
y = training_set$Purchased,
ntree = 10)
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[, 3], y_pred)
cm
View(training_set)
View(training_set)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Encoding the target feature (categorial variable) as factor, isso pe necessário para resvolver a questão do erro factor(0)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
#library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
classifier = randomForest(x = training_set[-3],
y = training_set$Purchased,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
cm
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Encoding the target feature (categorial variable) as factor, isso pe necessário para resvolver a questão do erro factor(0)
#dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
#library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
# Fitting the Random Forest Classification Model to the dataset
# install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-3],
y = training_set$Purchased,
ntree = 10)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
dataset$Purchased
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
dataset$Purchased
t = dataset$Purchased
classifier = randomForest(x = training_set[-3],
y = training_set$Purchased,
ntree = 500)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Encoding the target feature (categorial variable) as factor, isso pe necessário para resvolver a questão do erro factor(0). (the terms ‘category’ and ‘enumerated type’ are also used for factors)
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
#se não utilizarmos o método factor, o modelo tenta realizar uma regressão e exibe a mensagem abaixo
#Warning message:
#  In randomForest.default(x = training_set[-3], y = training_set$Purchased,  :
#  The response has five or fewer unique values.  Are you sure you want to do regression?
#Splitting the dataset into the Test set and the Training set
#install.packages('caTools')
#library(caTools) #ou pode selecionar o checbox
set.seed(123) #para ter os mesmos resultados
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
# scale é aplicado somente em números, porém ao usar o factor, as colunas 1 e 4 eram caracteres e depois trocadas para números, por isso pegamos somente as colunas 2 e 3
training_set[, 1:2] = scale(training_set[, 1:2])
test_set[, 1:2] = scale(test_set[, 1:2])
classifier = randomForest(x = training_set[-3],
y = training_set$Purchased,
ntree = 500)
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
cm
classifier = randomForest(x = training_set[-3],
y = training_set$Purchased,
ntree = 10)
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
cm
library(cowplot)#library(ElemSatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
View(training_set)
library("cowplot", lib.loc="~/R/win-library/3.5")
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
View(training_set)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.1)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.1)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X2), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '19', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = 19, col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
x1
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, 3],
main = 'Random Forest Classification (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
